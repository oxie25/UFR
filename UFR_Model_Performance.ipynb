{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2ec4dd3",
   "metadata": {},
   "source": [
    "### This workbook is used for evaluating UFR models and training additional molecules onto existing models\n",
    "\n",
    "**Oliver Xie - Olsen Lab, Massachusetts Institute of Technology, 2025**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2c12d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import warnings\n",
    "import ufr.util.data_processing as data_processing\n",
    "import ufr.util.model_launch as model_launch\n",
    "from ufr.model.idac_model_add_molecules import UNIQUAC_add_molecules, mod_UNIQUAC_add_molecules, Wilson_add_molecules, NRTL_add_molecules, EarlyStopper\n",
    "\n",
    "# Disable prototype warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "# Disable future deprecation warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Specify to use a specific GPU device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74edf5e3",
   "metadata": {},
   "source": [
    "### 1. Evaluate a trained model on IDAC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6f6d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the data file\n",
    "eval_data_file = \"./data/opensource_IDAC_data.csv\"\n",
    "prop_file = \"./data/small_molecule_prop.csv\"\n",
    "\n",
    "df_eval = pd.read_csv(eval_data_file, index_col = 0)\n",
    "df_prop = pd.read_csv(prop_file, index_col = 0)\n",
    "\n",
    "solute_smiles = 'Solute SMILES' # Specify the column name in df_eval\n",
    "solvent_smiles = 'Solvent SMILES' # Specify the column name in df_eval\n",
    "\n",
    "# Specify parts of the model\n",
    "combinatorial_layer = 'mod_FH' # Choose between FH, FV, SG, GK-FV, mod_FH, mod_FV\n",
    "residual_layer = 'mod_UNIQUAC' # Choose between UNIQUAC, mod_UNIQUAC, Wilson, NRTL\n",
    "association_layer = 'wertheim' # Choose between none, wertheim\n",
    "dimension = 12\n",
    "temp_type = 'invT'\n",
    "temp_dim = 2\n",
    "sobolev = 0\n",
    "trial = 5\n",
    "\n",
    "model_name = f'UFR_{combinatorial_layer}_{residual_layer}_{association_layer}_{dimension}D_{temp_type}_{temp_dim}_sobolev_{sobolev}_{trial}'\n",
    "model_file = f\"./trained_models/models_from_paper/{model_name}.h5\" # Change the filepath to the model file you want to use\n",
    "\n",
    "# We need to specify which IDAC is the correct one when removing the combinatorial layer - this is the IDAC to use for the slope with 1/T calculation\n",
    "ln_y_label = f'ln_gamma_res_{combinatorial_layer}' # Do it for just this for now\n",
    "comb_label = f'comb_{combinatorial_layer}' # This is name of the column containing the calculated combinatorial value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b69c56",
   "metadata": {},
   "source": [
    "**Clean data (if desired)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ba4cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Certain datasets report deuterated water but RDKit converts it to water. There are slight differences in IDAC for D2O. Manually convert these to a different SMILES name.\n",
    "deuterated_water = 'Deuterium oxide <Heavy water>'\n",
    "df_eval.loc[df_eval['Solute'] == deuterated_water, solute_smiles] = '[2H]O[2H]'\n",
    "df_eval.loc[df_eval['Solvent'] == deuterated_water, solvent_smiles] = '[2H]O[2H]'\n",
    "\n",
    "# Filter out duplicates and drop self-edges\n",
    "self_edges = df_eval[df_eval[solute_smiles] == df_eval[solvent_smiles]].index\n",
    "df_eval = df_eval.drop(self_edges)\n",
    "\n",
    "# Drop duplicates\n",
    "df_eval = data_processing.drop_duplicates(df_eval)\n",
    "\n",
    "# Set flag on whether to clean the data or not\n",
    "clean_temp_outliers = True # False or True\n",
    "\n",
    "# If True, clean the data\n",
    "if clean_temp_outliers:\n",
    "    # Load the cleaning rules\n",
    "    cleaning_rules_file = './data/cleaning_rules.xlsx'\n",
    "    df_clean_rules = pd.read_excel(cleaning_rules_file, sheet_name = 'cleaning rules')\n",
    "\n",
    "    # Clean according to the cleaning rules\n",
    "    df_eval = data_processing.apply_cleaning(df_eval, df_clean_rules)\n",
    "\n",
    "    # Drop all outliers. Can specify how many times away from the standard deviation to consider an outlier (default 3)\n",
    "    df_eval, df_dropped = data_processing.remove_temperature_outliers(df_eval, std_dev = 3) # df_dropped is what were dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66add384",
   "metadata": {},
   "source": [
    "**Add molecular properties and slope with temperature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ef219d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add molecular properties\n",
    "df_eval, v0, s0 = data_processing.molecular_property_addition(df_eval, df_prop, mode = 'FH', solute_smiles = solute_smiles, solvent_smiles = solvent_smiles) # The mode can be changed between 'FH' and 'FV'. Note: 'FV' requires the DIPPR correlations\n",
    "\n",
    "print('Starting calculation for Sobolev regularization, this might take a while')\n",
    "df_eval = data_processing.invtemp_gradient_calc(df_eval, ln_y_label, min_temps = 4, min_delta_T = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279b41c7",
   "metadata": {},
   "source": [
    "**Extract the model parameters and load the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0673c00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.HDFStore(model_file) as store:\n",
    "    df_results = store['df_chemical_parameters']\n",
    "    df_temp = store['df_temp_parameters']\n",
    "    df_loss = store['df_loss']\n",
    "    df_distance = store['df_distance']\n",
    "\n",
    "# Get the exponents used for this model\n",
    "temp_exponents = np.array(df_temp.columns, dtype = np.float64)\n",
    "\n",
    "# Fill in na with 0\n",
    "x_results = df_results.to_numpy()\n",
    "\n",
    "# Extract everything as numpy arrays to send into model\n",
    "T = df_temp.to_numpy()\n",
    "O = df_distance.to_numpy()\n",
    "\n",
    "# Set up the parameter arrays\n",
    "# Programatically access\n",
    "A = df_results.filter(like = 'ua_').to_numpy()\n",
    "Q = df_results.filter(like = 'q_').to_numpy()\n",
    "Alpha = df_results.filter(like = 'alpha_').to_numpy()\n",
    "Delta = df_results.loc[:, df_results.columns.str.contains('acceptor') | df_results.columns.str.contains('donor')].to_numpy()\n",
    "\n",
    "arrays = [arr for arr in (A, Alpha, Q, Delta) if arr.size > 0]\n",
    "\n",
    "# Get the PyTorch model\n",
    "model = model_launch.model_selector(residual_layer, A, Alpha, Q, T, O, Delta, association_layer, temp_exponents, device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97cdbbb",
   "metadata": {},
   "source": [
    "**Truncate the evaluated data to retain only IDAC with solutes and solvents present in the UFR model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1373cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get what chemicals are in df_results\n",
    "(node_size, model_dim) = df_results.shape\n",
    "index = df_results.index\n",
    "\n",
    "# Go through df_eval and drop all rows of data which we don't have a regressed value (solvent or solute).\n",
    "# Initialize a list to store the valid rows\n",
    "valid_rows = []\n",
    "\n",
    "for idx, row in df_eval.iterrows():\n",
    "    if row[solute_smiles] in index and row[solvent_smiles] in index:\n",
    "        valid_rows.append(idx)\n",
    "        solute_solvent = (row[solute_smiles], row[solvent_smiles])\n",
    "\n",
    "# Filter dfA to only include valid rows\n",
    "df_eval_filtered = df_eval.loc[valid_rows]\n",
    "\n",
    "print(f\"We retain only {df_eval_filtered.shape[0]} rows from the original {df_eval.shape[0]} representing {node_size} chemicals\")\n",
    "\n",
    "# We need to generate link all the chemicals in df_eval_filtered to the correct row of the parameter matrices\n",
    "# Use get_loc to find the index positions of the values in df_eval in df_results's index\n",
    "solute_positions = [index.get_loc(value) for value in df_eval_filtered[solute_smiles]]\n",
    "solvent_positions = [index.get_loc(value) for value in df_eval_filtered[solvent_smiles]]\n",
    "\n",
    "# Convert the list of positions to a numpy array\n",
    "solute_positions = np.array(solute_positions)\n",
    "solvent_positions = np.array(solvent_positions)\n",
    "\n",
    "# Set up data as tensors\n",
    "i_data = solute_positions\n",
    "j_data = solvent_positions\n",
    "invT_data = 1 / df_eval_filtered['Temp (K)'].to_numpy()\n",
    "rA_data = df_eval_filtered['Solute_VDW_Volumes'].to_numpy() / v0\n",
    "rB_data = df_eval_filtered['Solvent_VDW_Volumes'].to_numpy() / v0\n",
    "qA_data = df_eval_filtered['Solute_VDW_Area'].to_numpy() / s0\n",
    "qB_data = df_eval_filtered['Solvent_VDW_Area'].to_numpy() / s0\n",
    "weight_data = 1 / df_eval_filtered['Solute_Solvent_Count'].to_numpy()\n",
    "N_d_A = df_eval_filtered['Solute_H_Donor_Sites'].astype(float).to_numpy()\n",
    "N_a_A = df_eval_filtered['Solute_H_Acceptor_Sites'].astype(float).to_numpy()\n",
    "N_d_B = df_eval_filtered['Solvent_H_Donor_Sites'].astype(float).to_numpy()\n",
    "N_a_B = df_eval_filtered['Solvent_H_Acceptor_Sites'].astype(float).to_numpy()\n",
    "rhoBam = N_a_B / rB_data\n",
    "rhoBdm = N_d_B / rB_data\n",
    "rhoAap = N_a_A / rA_data\n",
    "rhoAdp = N_d_A / rA_data\n",
    "\n",
    "N_data = np.column_stack((N_a_A, N_d_A))\n",
    "rho_data = np.column_stack((rhoAap, rhoAdp, rhoBam, rhoBdm))\n",
    "\n",
    "# Push to GPU\n",
    "i = torch.tensor(i_data, dtype = torch.int32).to(device)\n",
    "j = torch.tensor(j_data, dtype = torch.int32).to(device)\n",
    "invT = torch.tensor(invT_data[:, None]).to(device)\n",
    "rA = torch.tensor(rA_data[:, None]).to(device)\n",
    "qA = torch.tensor(qA_data[:, None]).to(device)\n",
    "qB = torch.tensor(qB_data[:, None]).to(device)\n",
    "N = torch.tensor(N_data, dtype = torch.float32).to(device)\n",
    "rho = torch.tensor(rho_data, dtype = torch.float32).to(device)\n",
    "\n",
    "# Evaluate the model, require gradients to calculate the gradient with inverse temperature\n",
    "invT.requires_grad = True\n",
    "output = model(i, j, invT, rA, qA, qB, N, rho) # Pass q or r depending on the model\n",
    "output_dinvT = torch.autograd.grad(outputs=output, inputs= invT, grad_outputs=torch.ones_like(output), create_graph=True)[0]\n",
    "invT.requires_grad = False\n",
    "\n",
    "# Push to CPU\n",
    "ln_y_hat = output.detach().cpu().double().numpy().flatten()\n",
    "y_hat = np.exp(ln_y_hat)\n",
    "\n",
    "# Save the values into the DataFrame. \n",
    "df_eval_filtered['UFR_Model_ln_gamma_res'] = ln_y_hat\n",
    "df_eval_filtered['UFR_Model_derivative_1/T'] = output_dinvT.detach().cpu().double().numpy().flatten()\n",
    "df_eval_filtered['UFR_Model_ln_gamma'] = ln_y_hat + df_eval_filtered[comb_label]\n",
    "\n",
    "# Calculate the Aboslute\n",
    "df_eval_filtered['AE_ln_gamma'] = np.absolute(df_eval_filtered['ln gamma'] - df_eval_filtered['UFR_Model_ln_gamma'])\n",
    "df_eval_filtered['AE_derivative_1/T'] = np.absolute(df_eval_filtered['dlny_dinvT'] - df_eval_filtered['UFR_Model_derivative_1/T'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ab5b1b",
   "metadata": {},
   "source": [
    "**Model Performance Statistics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e950de0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean and median\n",
    "mean_AE = df_eval_filtered['AE_ln_gamma'].mean()\n",
    "median_AE = df_eval_filtered['AE_ln_gamma'].median()\n",
    "\n",
    "print(f\"The mean AE is {mean_AE} and the median AE is {median_AE}\")\n",
    "\n",
    "# Make a box and whiskers plot of the absolute error\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.set(font='Arial', font_scale=1.5)\n",
    "sns.set_style(\"whitegrid\")  # Set the background to white\n",
    "ax = sns.boxplot(y=df_eval_filtered['AE_ln_gamma'], data=df_eval_filtered, palette=\"viridis\", showfliers=False, medianprops={\"color\": \"white\", \"linewidth\": 2})\n",
    "\n",
    "# Set plot limits and labels\n",
    "plt.ylim(-0.01, 0.3)\n",
    "plt.ylabel('Absolute Error')\n",
    "plt.xlabel('Model')\n",
    "#plt.legend(title='Model')\n",
    "ax.spines['top'].set_color('black')\n",
    "ax.spines['bottom'].set_color('black') \n",
    "ax.spines['left'].set_color('black')\n",
    "ax.spines['right'].set_color('black')\n",
    "plt.rcParams.update({'font.family': 'Arial', 'font.size': 18})\n",
    "for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    label.set_fontsize(18)\n",
    "    label.set_fontname('Arial')\n",
    "    label.set_color('black')\n",
    "plt.gcf().set_size_inches(8, 6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae72627",
   "metadata": {},
   "source": [
    "**Cumulative error plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be5486e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the cumulative error\n",
    "\n",
    "mae_noise = {12: 0.0565} # For a 12-D model, this is determined on the experimental dataset used in training via the analysis in the SI.\n",
    "\n",
    "# Get the AE of each prediction\n",
    "ae_values = df_eval_filtered['AE_ln_gamma'].to_numpy()\n",
    "\n",
    "sort_ae_values = np.sort(ae_values)\n",
    "cumulative_prob = np.arange(1, len(sort_ae_values) + 1) / len(sort_ae_values)\n",
    "\n",
    "# Plot the cumulative probability as a line\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.plot(sort_ae_values, cumulative_prob, color='black', label = 'Model')\n",
    "\n",
    "# Extracting values\n",
    "\n",
    "# Set the title and labels\n",
    "#ax.set_title(\"Cumulative Probability Curve of AE\", fontsize=18, fontname='Arial', color='black')\n",
    "ax.set_xlabel(\"Absolute Error\", fontsize=18, fontname='Arial', color='black')\n",
    "ax.set_ylabel(\"Cumulative Probability\", fontsize=18, fontname='Arial', color='black')\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_yticks(np.arange(0, 1.1, 0.1))\n",
    "ax.grid(True)\n",
    "# Set the box around the plot area as black\n",
    "ax.spines['top'].set_color('black')\n",
    "ax.spines['bottom'].set_color('black')\n",
    "ax.spines['left'].set_color('black')\n",
    "ax.spines['right'].set_color('black')\n",
    "\n",
    "# Turn off for now, turn on when you have the noise limit and set it appropriately\n",
    "#ax.vlines(mae_noise[dimension], 0, 1, color='red', linestyle='--', label='Data Noise Limit')\n",
    "\n",
    "plt.legend(loc = 'lower right', prop = {'family': 'Arial', 'size': 18})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab628eb6",
   "metadata": {},
   "source": [
    "**Parity Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1af255c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a parity plot of the data\n",
    "# Make a parity plot\n",
    "plt.figure(figsize=(6,6))\n",
    "\n",
    "# Create colors array \n",
    "colors = ['blue' if (row[solute_smiles] == 'O' or row[solvent_smiles] == 'O') else 'green' \n",
    "          for _, row in df_eval_filtered.iterrows()]\n",
    "\n",
    "# Plot scatter points\n",
    "plt.scatter(df_eval_filtered['ln gamma'], df_eval_filtered['UFR_Model_ln_gamma'], c=colors, s=4, alpha=0.5)\n",
    "\n",
    "# Add diagonal line\n",
    "min_val = min(df_eval_filtered['ln gamma'].min(), df_eval_filtered['UFR_Model_ln_gamma'].min())\n",
    "max_val = max(df_eval_filtered['ln gamma'].max(), df_eval_filtered['UFR_Model_ln_gamma'].max())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'r--', label='Perfect Agreement')\n",
    "\n",
    "# Customize plot\n",
    "plt.xlabel('Experimental Data', fontsize=18, fontname='Arial')\n",
    "plt.ylabel('UFR Model Prediction', fontsize=18, fontname='Arial') \n",
    "\n",
    "# Set font properties for tick labels\n",
    "plt.xticks(fontsize=18, fontname='Arial')\n",
    "plt.yticks(fontsize=18, fontname='Arial')\n",
    "\n",
    "# Make plot square\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.grid(False)\n",
    "\n",
    "# Add legend with Arial font\n",
    "plt.legend(loc='upper left', prop={'family': 'Arial', 'size': 18})\n",
    "\n",
    "# Adjust spines\n",
    "ax = plt.gca()\n",
    "ax.spines['top'].set_color('black')\n",
    "ax.spines['bottom'].set_color('black')\n",
    "ax.spines['left'].set_color('black') \n",
    "ax.spines['right'].set_color('black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5200d1c2",
   "metadata": {},
   "source": [
    "### 2. Train additional chemicals onto the UFR model\n",
    "\n",
    "Additional data must be IDAC data of same format as original. Properties of the molecule must be included in the original property file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79809719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the data file\n",
    "train_data_file = \"./data/glycolide_IDAC_data.csv\"\n",
    "prop_file = \"./data/small_molecule_prop.csv\"\n",
    "\n",
    "df_train = pd.read_csv(train_data_file, index_col = 0)\n",
    "df_prop = pd.read_csv(prop_file, index_col = 0)\n",
    "\n",
    "solute_smiles = 'Solute SMILES' # Specify the column name in df_eval\n",
    "solvent_smiles = 'Solvent SMILES' # Specify the column name in df_eval\n",
    "\n",
    "# Specify parts of the model\n",
    "combinatorial_layer = 'mod_FH' # Choose between FH, FV, SG, GK-FV, mod_FH, mod_FV\n",
    "residual_layer = 'mod_UNIQUAC' # Choose between UNIQUAC, mod_UNIQUAC, Wilson, NRTL\n",
    "association_layer = 'wertheim' # Choose between none, wertheim\n",
    "dimension = 12\n",
    "temp_type = 'invT'\n",
    "temp_dim = 2\n",
    "sobolev = 0\n",
    "trial = 5\n",
    "\n",
    "model_name = f'UFR_{combinatorial_layer}_{residual_layer}_{association_layer}_{dimension}D_{temp_type}_{temp_dim}_sobolev_{sobolev}_{trial}'\n",
    "model_file = f\"./trained_models/models_from_paper/{model_name}.h5\" # Change the filepath to the model file you want to use\n",
    "\n",
    "# We need to specify which IDAC is the correct one when removing the combinatorial layer - this is the IDAC to use for the slope with 1/T calculation\n",
    "ln_y_label = f'ln_gamma_res_{combinatorial_layer}' # Do it for just this for now\n",
    "comb_label = f'comb_{combinatorial_layer}' # This is name of the column containing the calculated combinatorial value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed277f4",
   "metadata": {},
   "source": [
    "**Add molecular properties**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2f131c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add molecular properties\n",
    "df_train, v0, s0 = data_processing.molecular_property_addition(df_train, df_prop, mode = 'FH', solute_smiles = solute_smiles, solvent_smiles = solvent_smiles) # The mode can be changed between 'FH' and 'FV'. Note: 'FV' requires the DIPPR correlations\n",
    "\n",
    "print('Starting calculation for Sobolev regularization, this might take a while')\n",
    "df_train = data_processing.invtemp_gradient_calc(df_train, ln_y_label, min_temps = 4, min_delta_T = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84aae937",
   "metadata": {},
   "source": [
    "**Set up the model**\n",
    "\n",
    "Most of the model parameters are defined already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2148498f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a model savepath\n",
    "save_path = './trained_models/'\n",
    "\n",
    "# Set number of trials to run. This number serves as the seed to the random number generator.\n",
    "trials = 1\n",
    "\n",
    "# Load in a model.\n",
    "with pd.HDFStore(model_file) as store:\n",
    "    df_results = store['df_chemical_parameters']\n",
    "    df_temp = store['df_temp_parameters']\n",
    "    df_loss = store['df_loss']\n",
    "    df_distance = store['df_distance']\n",
    "\n",
    "# Get the exponents used for this model\n",
    "temp_exponents = np.array(df_temp.columns, dtype = np.float64)\n",
    "\n",
    "# Fill in na with 0\n",
    "x_results = df_results.to_numpy()\n",
    "\n",
    "# Extract everything as numpy arrays to send into model\n",
    "T = df_temp.to_numpy()\n",
    "O = df_distance.to_numpy()\n",
    "\n",
    "# Set up the parameter arrays - keep as dataframe\n",
    "# Programatically access\n",
    "A = df_results.filter(like = 'ua_')\n",
    "Q = df_results.filter(like = 'q_')\n",
    "Alpha = df_results.filter(like = 'alpha_')\n",
    "Delta = df_results.loc[:, df_results.columns.str.contains('acceptor') | df_results.columns.str.contains('donor')]\n",
    "\n",
    "arrays = [arr for arr in (A, Alpha, Q, Delta) if arr.size > 0]\n",
    "\n",
    "# Get the list of chemicals already contained in this model\n",
    "regressed_smiles = df_results.index.values\n",
    "\n",
    "# Get the dimensions\n",
    "alpha_dim = Alpha.shape[1] if A.size > 0 else 0\n",
    "q_dim = Q.shape[1] if Q.size > 0 else 0\n",
    "delta_dim = Delta.shape[1] if Delta.size > 0 else 0\n",
    "u_dim = A.shape[1] if A.size > 0 else 0\n",
    "\n",
    "# Sobolev loss - Currently only allowing 0 when adding new molecules\n",
    "sobolev = 0 # 0 turns this off. Any non-zero value turns on the Sobolev loss and becomes the weight for the term.\n",
    "\n",
    "# Hyperparameters\n",
    "lr = 0.01 # Learning rate\n",
    "total_epochs = 30000 # Total number of epochs to run\n",
    "up_epochs = 1000 # Number of epochs for ramping up the model\n",
    "hold_epochs = 20000 # Number of epochs for holding at the maximum learning rate\n",
    "pre_train_epoch = 500 # Number of epochs for separately regressing the residual and association layers if both are used.\n",
    "\n",
    "# Truncation to prevent overfitting\n",
    "truncation = 'chemical_connections' # Set to temp_connections if we want to count each pairing's temperature as unique. Set to chemical_connections if we want to count each pairing's temperature as one entry. Set to 'keep_all' if we don't want it to truncate\n",
    "\n",
    "# Set up a savename for the model\n",
    "model_name = f'UFR_{combinatorial_layer}_{residual_layer}_{association_layer}_{dimension}D_{temp_type}_{temp_exponents.size}D_sobolev_{sobolev}_Additional_Molecule'\n",
    "save_name = save_path + model_name\n",
    "\n",
    "# Set up dictionaries of model parameters for passing into the model\n",
    "ln_y_data = f'ln_gamma_res_{combinatorial_layer}' # This specifies which IDAC with combinatorial removed to regress on.\n",
    "model_layer_options = {'ln_y_data': ln_y_data, 'combinatorial_layer': combinatorial_layer, 'residual_layer': residual_layer, 'association_layer': association_layer, 'temp_exponents': temp_exponents, 'reference_volume': v0, 'reference_area': s0}\n",
    "model_opt_options = {'sobolev': sobolev, 'lr': lr, 'total_epochs': total_epochs, 'up_epochs': up_epochs, 'hold_epochs': hold_epochs}\n",
    "model_run_options = {'truncation': truncation, 'smile_labels' : (solute_smiles, solvent_smiles), 'pre_train_epoch': pre_train_epoch, 'save_name': save_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00600b7",
   "metadata": {},
   "source": [
    "**Run the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f8f51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model\n",
    "# Set up the optimization\n",
    "criterion = nn.L1Loss() # Use MAE as loss criteria\n",
    "\n",
    "# Generate a list of unique nodes in the new dataset we want to correlate\n",
    "full_node_list = list(pd.unique(df_train[[solute_smiles, solvent_smiles]].values.ravel('K')))\n",
    "# Create a mapping of node to index\n",
    "node_idx_dict = dict(zip(np.array(full_node_list), np.array(range(np.size(full_node_list)))))\n",
    "# Generate a mask for masking all nodes which are already trained in the model, so that they are not regressed again\n",
    "node_mask = np.array([False if node in regressed_smiles else True for node in full_node_list])\n",
    "\n",
    "# Add index columns for solute and solvent\n",
    "df_train['Solute_Idx'] = df_train[solute_smiles].map(node_idx_dict)\n",
    "df_train['Solvent_Idx'] = df_train[solvent_smiles].map(node_idx_dict)\n",
    "\n",
    "edges = df_train.shape\n",
    "n_node = np.size(full_node_list)  # full graph node size\n",
    "\n",
    "# Set up data as tensors\n",
    "i_data = df_train['Solute_Idx'].to_numpy()\n",
    "j_data = df_train['Solvent_Idx'].to_numpy()\n",
    "invT_data = 1 / df_train['Temp (K)'].to_numpy()\n",
    "y_data = df_train[ln_y_data].to_numpy()\n",
    "rA_data = df_train['Solute_VDW_Volumes'].to_numpy() / v0\n",
    "rB_data = df_train['Solvent_VDW_Volumes'].to_numpy() / v0\n",
    "qA_data = df_train['Solute_VDW_Area'].to_numpy() / s0\n",
    "qB_data = df_train['Solvent_VDW_Area'].to_numpy() / s0\n",
    "N_d_A = df_train['Solute_H_Donor_Sites'].astype(float).to_numpy()\n",
    "N_a_A = df_train['Solute_H_Acceptor_Sites'].astype(float).to_numpy()\n",
    "N_d_B = df_train['Solvent_H_Donor_Sites'].astype(float).to_numpy()\n",
    "N_a_B = df_train['Solvent_H_Acceptor_Sites'].astype(float).to_numpy()\n",
    "rhoBam = N_a_B / rB_data\n",
    "rhoBdm = N_d_B / rB_data\n",
    "rhoAap = N_a_A / rA_data\n",
    "rhoAdp = N_d_A / rA_data\n",
    "N_data = np.column_stack((N_a_A, N_d_A))\n",
    "rho_data = np.column_stack((rhoAap, rhoAdp, rhoBam, rhoBdm))\n",
    "\n",
    "np.random.seed(1) # For reproducibility across trials \n",
    "\n",
    "early_stopping = EarlyStopper() # Set up early stopper\n",
    "\n",
    "# Push to GPU\n",
    "i = torch.tensor(i_data).to(device)\n",
    "j = torch.tensor(j_data).to(device)\n",
    "invT = torch.tensor(invT_data[:, None]).to(device) # Must be Ndata x 1\n",
    "y = torch.tensor(y_data[:, None]).to(device)\n",
    "rA = torch.tensor(rA_data[:, None]).to(device)\n",
    "qA = torch.tensor(qA_data[:, None]).to(device)\n",
    "qB = torch.tensor(qB_data[:, None]).to(device)\n",
    "N = torch.tensor(N_data, dtype = torch.float32).to(device)\n",
    "rho = torch.tensor(rho_data, dtype = torch.float32).to(device)\n",
    "\n",
    "q = df_prop.set_index('Canonical SMILES').loc[full_node_list, 'van der waals area (m2/kmol)'].to_numpy() / s0\n",
    "\n",
    "loss_values = []\n",
    "# Initialize model\n",
    "Alpha_shape = (n_node, alpha_dim)\n",
    "A_shape = (n_node, u_dim)\n",
    "T_shape = (u_dim, temp_dim)\n",
    "D_shape = (n_node, delta_dim) # Always pass in Wertheim, won't always use\n",
    "Q_shape = (n_node, 1)\n",
    "O_shape = (u_dim, 2) # For combining the u_dim parameters\n",
    "\n",
    "# Guess, then overwrite with the actual values\n",
    "A_initial = 0.1 * 20 * np.random.standard_normal(size = A_shape) + 20 # This makes all values large, and makes it 10% of the guess size\n",
    "Alpha_initial = np.random.random_sample(size = Alpha_shape) # From [0, 1)\n",
    "D_initial = 0.1 * np.random.standard_normal(size = D_shape) + np.log(np.exp(1) - 1) * np.ones(D_shape) # Narrow random distribution around 1 (after softmax)\n",
    "Q_initial = np.zeros(Q_shape) #0.1 * np.random.standard_normal(size = Q_shape) + 1 # This must be q to start\n",
    "Q_initial[:, 0] = q.copy()\n",
    "\n",
    "# Overwrite with regressed values\n",
    "for smiles, idx in node_idx_dict.items():\n",
    "    if smiles in regressed_smiles:\n",
    "        A_initial[idx, :] = A.loc[smiles].to_numpy()\n",
    "        Alpha_initial[idx, :] = Alpha.loc[smiles].to_numpy()\n",
    "        D_initial[idx, :] = Delta.loc[smiles].to_numpy()\n",
    "        Q_initial[idx, :] = Q.loc[smiles].to_numpy()\n",
    "\n",
    "# Push to gpu\n",
    "A_initial = torch.tensor(A_initial, dtype=torch.float32).to(device)\n",
    "Alpha_initial = torch.tensor(Alpha_initial, dtype=torch.float32).to(device)\n",
    "D_initial = torch.tensor(D_initial,  dtype=torch.float32).to(device)\n",
    "T_initial = torch.tensor(df_temp.to_numpy(), dtype=torch.float32).to(device) # From regressed results\n",
    "O_initial = torch.tensor(df_distance.to_numpy(), dtype=torch.float32).to(device) # From regressed results\n",
    "Q_initial = torch.tensor(Q_initial, dtype = torch.float32).to(device)\n",
    "\n",
    "node_mask_tensor = torch.tensor(node_mask).to(device)\n",
    "\n",
    "# Select model here\n",
    "# Select model here\n",
    "if residual_layer == 'UNIQUAC':\n",
    "    model = UNIQUAC_add_molecules(A_initial, T_initial, O_initial, D_initial, association_layer, temp_exponents, node_mask_tensor, device)\n",
    "elif residual_layer == 'Wilson':\n",
    "    model = Wilson_add_molecules(A_initial, T_initial, O_initial, D_initial, association_layer, temp_exponents, node_mask_tensor, device)\n",
    "elif residual_layer == 'NRTL':\n",
    "    model = NRTL_add_molecules(Alpha_initial, A_initial, T_initial, O_initial, D_initial, association_layer, temp_exponents, node_mask_tensor, device)\n",
    "elif residual_layer == 'mod_UNIQUAC':\n",
    "    model = mod_UNIQUAC_add_molecules(A_initial, Q_initial, T_initial, O_initial, D_initial, association_layer, temp_exponents, node_mask_tensor, device)\n",
    "else:\n",
    "    print(f'Model label of {residual_layer} is invalid')\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "# Change learning rate\n",
    "# Warm up for learning rate - use 1000 iterations to reach full loss\n",
    "scheduler = optim.lr_scheduler.LinearLR(optimizer, start_factor = 0.1, end_factor = 1, total_iters = 1000) # 1000 iterations to reach full LR of 0.01\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(total_epochs):\n",
    "    # Put model into training\n",
    "    model.train()\n",
    "    # Reset the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(i, j, invT, rA, qA, qB, N, rho)\n",
    "\n",
    "    # Compute the loss between predicted and target values without Sobolev regularization\n",
    "    loss = criterion(torch.squeeze(outputs), torch.squeeze(y))\n",
    "\n",
    "    # Backward pass, comput gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # Change learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # Append loss to the array\n",
    "    loss_values.append(loss.item())\n",
    "\n",
    "    early_stopping(loss.item())\n",
    "    # Print message every 500th iteration\n",
    "    if (epoch + 1) % 500 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{total_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(f\"Early stopping occurred at Epoch [{epoch + 1}/{total_epochs}], Loss: {loss.item():.4f}\")\n",
    "        break\n",
    "\n",
    "# Transfer tensors to array - remember we need to select between the learned and fixed values appropriately\n",
    "# All models have A, O, T\n",
    "A_learn = model.A.detach().cpu()\n",
    "A_fixed = model.A_fixed.detach().cpu()\n",
    "A_final = torch.where(node_mask_tensor.cpu().unsqueeze(1), A_learn, A_fixed).numpy()\n",
    "# T and O are always fixed and should be original model values\n",
    "T_final = model.T.detach().cpu().numpy()\n",
    "O_final = model.O.detach().cpu().numpy()\n",
    "Alpha_final = np.array([])\n",
    "Q_final = np.array([])\n",
    "D_final = np.array([])\n",
    "\n",
    "# Extract alpha for NRTL, Q for mod-UNIQUAC\n",
    "if residual_layer == 'NRTL':\n",
    "    Alpha_learn = model.Alpha.detach().cpu()\n",
    "    Alpha_fixed = model.Alpha_fixed.detach().cpu()\n",
    "    Alpha_final = torch.where(node_mask_tensor.cpu().unsqueeze(1), Alpha_learn, Alpha_fixed).numpy()\n",
    "elif residual_layer == 'mod_UNIQUAC':\n",
    "    Q_learn = model.Q.detach().cpu()\n",
    "    Q_fixed = model.Q_fixed.detach().cpu()\n",
    "    Q_final = torch.where(node_mask_tensor.cpu().unsqueeze(1), Q_learn, Q_fixed).numpy()\n",
    "\n",
    "# Extract Delta for Wertheim\n",
    "if association_layer == 'wertheim':\n",
    "    D_learn = model.D.detach().cpu()\n",
    "    D_fixed = model.D_fixed.detach().cpu()\n",
    "    D_final = torch.where(node_mask_tensor.cpu().unsqueeze(1), D_learn, D_fixed).numpy()\n",
    "    \n",
    "# Save the model parameters using state_dict, it is a .pt file\n",
    "model_filename = f'{save_name}_{trial}.pt'\n",
    "torch.save(model.state_dict(), model_filename)\n",
    "\n",
    "# Save the model parameters as h5 file format. We need to merge the array first\n",
    "param_array = [arr for arr in (A_final, Alpha_final, Q_final, D_final) if arr.size > 0]\n",
    "merged_array = np.hstack(param_array)\n",
    "\n",
    "# Array index\n",
    "index_series = pd.Series(node_idx_dict)\n",
    "\n",
    "df_results_add_mol = pd.DataFrame(merged_array, index = index_series.index, columns = df_results.columns)\n",
    "df_distance_add_mol = pd.DataFrame(O)\n",
    "df_temp_add_mol = pd.DataFrame(T, columns = temp_exponents)\n",
    "df_loss_add_mol = pd.DataFrame(loss_values)\n",
    "\n",
    "# Update the original df_results with the new values\n",
    "df_results_final = pd.concat([df_results, df_results_add_mol]).groupby(level=0).last()\n",
    "\n",
    "# Save all DataFrames as an HDF5 file\n",
    "h5_filename = f'{save_name}_{trial}.h5'\n",
    "with pd.HDFStore(h5_filename) as store:\n",
    "    store.put('df_chemical_parameters', df_results_final)\n",
    "    store.put('df_temp_parameters', df_temp_add_mol)\n",
    "    store.put('df_loss', df_loss_add_mol)\n",
    "    store.put('df_distance', df_distance_add_mol)\n",
    "\n",
    "print(f'Model saved as {model_filename} and {h5_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ef664a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataScience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
